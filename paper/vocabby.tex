%
% File acl2018.tex
%
%% Based on the style files for ACL-2017, with some changes, which were, in turn,
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2018}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Vocabby: Vocabulary learning with in the context of user domain}

\author{Haemanth Santhi Ponnusamy \\
  Department of computational linguistics, \\
  Eberhard Karls University of Tuebingen,\\
  {\tt hameanthsp@gmail.com} \\\And
  Himanshu Bansal \\
  Department of computational linguistics, \\
  Eberhard Karls University of Tuebingen,\\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  This is an vocabulary learning application for a targeted set of user
  who already posses some basic skills of the English language and
  bored of the traditional method of learning new word in the language. This
  application provides the ability to upload text of user preference
  such as a story book, research writing etc... All activities generated for
  vocabulary learning and their distractor are closed to the chosen content.
\end{abstract}

\section{Introduction}
The vocabulary learning is a never ending task. Since the languages are vast
and continuously evolving. Majority of the tools for vocabulary learning takes
the user through a pre defined set of word groups which is constructed into a
hierarchy of words in the language.(motivation) The main drawback of this approach are, it
takes too long for someone to get a feel of progress, it might not be the
order in which the user wants to learn words and the words might not be
introduced in the context that the user is not interested in. All the above reasons
makes the user feel boring and finally quit the habit of learning in few days
or weeks. (content creation) In order to keeps the user of different kind
motivated, the developer has to manually/automatically generate lot of content
to show the examples to all the words in the language in multiple context.
We attempt to overcome these issues of creating tool for vocabulary learning
by allowing the user to choose content their interest and we use that to
generate the content for the vocabulary learning activities. By this way one
could choose to improve vocabularies used in a specific domain with specific
context. They could move on to a new area once they complete that.
(Indefinite space) So the problem of vocabulary learning scales down from an
indefinite space to a finite space.

\section{Related works}
Details of some of the existing tools goes here.


\section{Our application}

\subsection{Building vocabulary}
The vocabulary is built from the user text.

\subsubsection{Candidates}
All the words in the user text doesn't need to be addressed. For examples 
function words (the, of, in, on etc...), rare/very less frequent words and
improperly parsed words. So the words occurring below the frequency of 10 and
the stop words(\url{https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py}) are eliminated. 

Also in-order to differentiate between the same words occurring in the
different parts of speech. The words are represented as a tuple of word and its
POS tag.
\begin{center}(word, POS tag)\end{center}

\subsubsection{Occurrence map}
All the sentences in which the candidate words occur are cleaned and mapped to
the corresponding words. So each candidate word is mapped to all the sentences
in which it occur and also each sentence is mapped to all the all the candidate
words it contains.

\subsubsection{Complexity}
There are lot of methods to measure the complexity of a word. We choose frequency.
In a language, we consider the words that are less frequently used are as
more complex and the words that occurs more frequent are as easier or well known.
We use the SUBTLEX-US~\cite{brysbaert2009moving}, a database of 50 million
words containing word frequencies based on English-US movies and TV series subtitles.
\begin{equation}
  C_w = \frac{1}{\log_{10}(freq_w)}
\end{equation}

Where the ${freq_w}$ in from this database is the average frequency of words per
million words.

\subsection{Creating structure}

\subsubsection{Family}
We group the words into families, similar to ~\citep{bauer1993word} work on word
families but instead of seven sub-groups we form a single group for all types.
This is based on the intuition that the user can extrapolate their knowledge of
inflections of a word to understand/predict all the forms of a words family
by knowing only one or few of them.

\subsubsection{Network}
Now the families has to co-exist in the space of language(conditioned by the
book) as a network like a society. It is a fully connected network with each
family with a different affinity to another. The affinity is a measure of
contextual similarity between the families. We compute cosine similarity 
between the mean of word vectors of all the members of the family to similar
mean vector representation of another family.

\begin{equation}
  V_{F_i} = \frac{1}{n} \sum_{k}^{n} V_{w_k}
\end{equation}

where ${n}$ is the number of elements of the family ${F_i}$.

\begin{equation}
  S_{ij} = \frac{V_{F_i} * V_{F_j}}{\|V_{F_i}\|  \|V_{F_j}\|}
\end{equation}

where, ${S_{ij}}$ is the cosine similarity between the vector representations of
families ${F_i}$ and ${F_j}$.


By this way we create more structure in the space of vocabulary. Which come
handy in many situations like activity creation, updating mastery of each
vocabulary, analysis etc... This structure helps in reducing the search space
by allowing the model to get a better inference about learners level with
relatively very less and effective interactions compared to a method of
tracking each word in the vocabulary individually.

\subsection{Content organization}
\subsubsection{Books}
All the processed data such as vocabulary, families, network, sample sentences
are packed into a book instance. This also tracks the meta information such as
Title, Author, Genre, Year and Publisher. This help any future user to select
the processed book directly. This also could help in comparing the performance
of different users on the same book.

\subsubsection{Bookshelf}
All such processed book are organized in multiple bookshelves specific to each
domain similar to the gutenberg project (\url{http://www.gutenberg.org}).

\subsection{Model}
In this work we maintain multiple models to track and update different aspect
of the application.

\subsubsection{Learner}
The learner instance maintain the personal information of the user and tracks
the list of instance of books the user has choose to improve vocabulary and the
progress in them. This also could maintain overall vocabulary knowledge of
the user and customize the activity type, feedback and book suggestions based
on individual needs.

\subsubsection{Tutor}
For each book the user selects, a tutor instance will be created. The main
activities of the tutor are to design a learning session, evaluate the
performance and track the mastery level of the user w.r.t all the vocabulary
in the book. Then again generate a new session based on the updated mastery
levels.

\textbf{Mastery Score:} The value ranges between 0 and 1. Initially it is
assigned to 0.5 to indicate the uncertainty. Based on the performance of the
user it is either increased or decreased by a factor. This approach implicitly
capture the un-visited nodes in the network.

\textbf{Update rule:} As we have built a network of families capturing the
contextual similarity. We can incorporate this into our update rule to update
the mastery scores. When we get some outcome for an activity involving a member
from the family ${F_i}$.

\begin{equation}
  M_j = M_j * (1 + (\alpha * sign * S_{ij}))
\end{equation}

Where ${M_i}$ is the mastery of the family ${i}$. ${\alpha}$ is a tunable parameter
for the magnitude of an update. ${sign \epsilon \{-1, +1\}}$ is the direction 
of the update. It depends on the correctness of user response to the corresponding
activity. And ${S_{ij}}$ is the measure of contextual similarity between the two
families.

\subsubsection{Session}
An instance of session is created by the tutor. A session is a list of word
families to be learned and evaluated in a session. The key functions of this
model are to deciding the interaction type (teaching, testing, feedback), 
compose an interaction with all required data and handles the flow and closure
of the session.

\subsection{Activity}
We create two type of activities. 

\section{Implementation}

\section*{Acknowledgments}

% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2018}
\bibliography{acl2018}
\bibliographystyle{acl_natbib}
\end{document}
